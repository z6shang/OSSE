import sys, os
import random 
from collections import defaultdict
import numpy as np 
from multiprocessing import Pool
import json
import time
import hashlib
import config 
import time 
import ipe_wrap 

class dp_sse_plaintext: 
    # Init parameters
    def __init__(self):
        # init parameters from config.py
        self.db_size = config.db_size
        self.new_db_size = config.new_db_size # when size_lmt = 300
        self.cmax = config.cmax
        self.smax = config.smax
        self.placeholder_in_index = config.placeholder_in_index
        self.large_p = config.large_p
        self.stopwords_size = config.large_p
        self.countermax = config.countermax
        self.db_path = config.db_path
        self.core_num = config.core_num
        self.max_len = config.max_len
        self.p1_len = config.p1_len 
        self.p2_len = config.p2_len 
        self.p3_len = config.p3_len 
        # uncomment the following 3 lines and the function self.gen_tokens_encrypted and self.gen_polynomial_encrypted to enable dp_sse_encrypted
        '''
        self.ipe = ipe_wrap.ipe_wrap( self.smax + 3 )
        self.ipe.init_para()
        self.ipe.para_setup()
        '''
        #init other parameters
        self.counter_map = defaultdict(int)
        # A map string => integer
        # map keyword + hash(keyword) to the current number of such string 
        self.counter_map_2_hash = defaultdict(int)
        self.sk = ""

    # set random seed for random generation
    def set_random_seed(self):
        random.seed( time.time() )


    # hash a keyword string into an integer (160bits), no modular
    # Input: 
    #   a: string 
    # Output:
    #   160-bit integer 
    ##
    def hash(self, a):
        return int(hashlib.sha1(str(a).encode('utf8')).hexdigest(), 16)
    
    # hash an integer into one of the self.cmax buckets
    # Input: 
    #   id: integer/string of int-like
    # Output:
    #   string of fixed length self.p2_len 
    ##
    def hash_1(self, id):
        random.seed( int(id) )
        pos =  random.randint(1, self.cmax)
        return str(pos).zfill( self.p2_len )
    
    # hash an integer into the better choice of the 2 buckets 
    # where the 2 candidate buckets are chosen uniformly at random 
    # "better" means that less have chosen such bucket.

    # Another function that hash an integer into one of the self.cmax buckets
    # Input: 
    #   id: integer/string of int-like
    # Output:
    #   string of fixed length self.p2_len 
    ##
    def hash_2(self, id):
        #return self.hash_1(id) #make everything run in one-hash setting
        #comment the above line to run in 2 hash setting.
        random.seed( int( id ) )
        random.seed( random.randint( 1, sys.maxsize ) )
        pos = random.randint(1, self.cmax)
        return str(pos).zfill( self.p2_len )
    
    # Convert keyword string into integer of fixed length p1_len 
    # Note: all keyword should be first convert into int-like string/int
    # Input: 
    #   ss: string of arbitrary length 
    # Output:
    #   string of fixed length, p1_len 
    ## 
    def p_keyword(self, ss):
        _ss = str( ss )[ 0 : self.max_len ]
        res = str( hash( _ss ) % ( 10**self.p1_len ) ).zfill( self.p1_len )
        return res
    
    # Given a keyword and global maps, choose the better bucket position and 
    # the corresponding counter in that bucket position 
    # Input:
    #   k : keyword, string , generated by p_keyword
    #   h1: string, produced by self.hash_1 
    #   h2: string, produced by self.hash_2 
    # Output:
    #   counter, string of fixed length self.p3_len 
    #   bucket number, string of fixed length self.p2_len 
    ##
    def p_counter(self, k, h1, h2):
        self.set_random_seed()
        res = (0, 0)
        if self.counter_map_2_hash[k+h1] < self.counter_map_2_hash[k+h2]:
            res = (k + h1, h1)
        elif self.counter_map_2_hash[k+h1] > self.counter_map_2_hash[k+h2]:
            res = (k + h2, h2)
        else:
            res = (k + h1, h1) if random.random() < 0.5 else (k + h2, h2)
        self.counter_map_2_hash[res[0]] += 1
            #counter start from 1
        return str(self.counter_map_2_hash[res[0]]).zfill(self.p3_len), res[1]

    # Gen the term encoding a keyword 
    # Input:
    #   keyword: string
    #   id: integer/string 
    # Output:
    #   integer mod self.large_p 
    ## 
    def gen_term_basic_2_hash_keyword(self, keyword, id):
        h1 = self.hash_1(id)
        h2 = self.hash_2(id)
        p1 = self.p_keyword(keyword)
        p3, p2 = self.p_counter(p1, h1, h2)
        term = p1 + p2 + p3
        return hash(term) % self.large_p 
    # Gen the term that used to pad a small file into size self.smax 
    # Input: 
    #   N/A 
    # Output:
    #   integer mod self.large_p
    # Note: As long as the encoded message is different from meaningful terms.
    ##
    def gen_term_basic_2_hash_padding(self):
        p1 = '1'.zfill( self.p1_len )
        p2 = '0'.zfill( self.p2_len )
        p3 = '0'.zfill( self.p3_len )
        term = p1 + p2 + p3
        return hash(term) % self.large_p 
    
    # Gen the term that used generate false positives
    # Input: 
    #   id: integer/string  
    # Output:
    #   integer mod self.large_p
    # Note: As long as the encoded message is different from other terms 
    # that encode real keywords/dummy terms for padding.
    ##
    def gen_term_basic_2_hash_id(self, id):
        p1 = str(id).zfill( self.p1_len )
        p21 = str(self.hash_1( id ) ).zfill( self.p2_len )
        p22 = str(self.hash_2( id ) ).zfill( self.p2_len )
        p3 = '100'.zfill( self.p3_len )
        term1 = p1 + p21 + p3
        term2 = p1 + p22 + p3
        return hash(term1) % self.large_p, hash(term2) % self.large_p 
    
    # Given a root, extend it into its series of power self.smax + 2 to 0
    # Input:
    #   term: integer/string of int-like, generate by gen_* functions
    # Output:
    #   A list of integers, self.smax + 3 in total 
    ##
    def poly_extend(self, term):
        term = int(term)
        poly = [
                    term**(self.smax+2 - i) % self.large_p 
                    for i in range(self.smax + 2)
                ]
        poly.append(1)
        return poly
    
    # Gen the search token for one keyword in one specific position(bucket and counter)
    # Input: 
    #   keyword: string 
    #   bucket:  int/string 
    #   counter: int/string 
    # Output:
    #    a list of self.smax + 3 integer mod self.large_p 
    ##
    def gen_token_basic_keyword(self, keyword, bucket, counter):
        p1 = self.p_keyword( keyword )
        p2 = str(bucket).zfill( self.p2_len )
        p3 = str(counter).zfill( self.p3_len )
        term = p1 + p2 + p3 
        term = hash(term) % self.large_p
        return self.poly_extend( term )
    
    # Gen dummy token that matches none 
    # Input: 
    #   N/A
    # Output:
    #   a list of self.smax + 3 integers mod self.large_p 
    ##
    def gen_token_basic_padding(self):
        p1 = '0'.zfill( self.p1_len )
        p2 = '2'.zfill( self.p2_len )
        p3 = '100'.zfill( self.p3_len )
        term = p1 + p2 + p3
        term = hash(term) % self.large_p
        return self.poly_extend( term )        
    
    # Gen false positive token
    # Input: 
    #   id: integer/string
    # Output:
    #   a list of self.smax + 3 integers mod self.large_p 
    ##
    def gen_token_basic_id_hash_1(self, id):
        p1 = str(id).zfill( self.p1_len )
        p2 = str( self.hash_1(id) ).zfill( self.p2_len )
        p3 = '100'.zfill( self.p3_len )
        term = p1 + p2 + p3
        term = hash(term) % self.large_p
        return self.poly_extend( term )        
    
    # Gen false positive token
    # Input: 
    #   id: integer/string
    # Output:
    #   a list of self.smax + 3 integers mod self.large_p 
    ##
    def gen_token_basic_id_hash_2(self, id):
        p1 = str(id).zfill( self.p1_len )
        p2 = str( self.hash_2(id) ).zfill( self.p2_len )
        p3 = '100'.zfill( self.p3_len )
        term = p1 + p2 + p3
        term = hash(term) % self.large_p
        return self.poly_extend( term )        
        
    
    # Given a list of keywords in a file, generate the corresponding polynomial roots
    # Input:
    #   keywords: a list of keywords (string)
    #   id: the id of the file (string/integer)
    # Output:
    #   np.array, all roots of the polynomial
    #   self.smax + 2 in total 
    ##
    def gen_polynomial_roots(self, keywords, id):
        term_list = []
        for keyword in keywords:
            term_list.append( self.gen_term_basic_2_hash_keyword(  keyword, id ) )
        for i in range( self.smax -  len(keywords) ):
            term_list.append( self.gen_term_basic_2_hash_padding( ) )
        term_list.extend( self.gen_term_basic_2_hash_id(id) )
        term_list = np.array( term_list, dtype=object )
        return term_list

    # Given all root, gen the polynomial represented by its coefficients
    # # Input:
    #   roots: np.array(, dtype = object)
    # Output:
    #   np.array, all coefficients of the polynomial 
    ##
    def gen_polynomial_from_roots(self, roots):
        poly = np.poly1d(roots, True) # coefficients from high degree to low
        coeffs = poly.coeffs % ( self.large_p)
        coeffs = coeffs.tolist()
        return coeffs
    
    # Given a list of keywords in a file, generate the corresponding index of the file
    # Input:
    #   keywords: a list of keywords (string)
    #   id: the id of the file (string/integer)
    # Output:
    #   list of integer mod self.large_p, all coefficients of the polynomial
    #   self.smax +3 in total 
    ##
    def gen_polynomial_plain(self, keywords, id):
        roots = self.gen_polynomial_roots(keywords, id)
        return self.gen_polynomial_from_roots( roots )
    
    '''
    def gen_polynomial_encrypted(self, poly):
        return self.ipe.encrypt_polycoeffs( poly )    
    '''

    # Given an index idx and a query token, return match (True) or not (False)
    # Input:
    #   idx: np.array(dtype = object), the polynomial coefficients
    #   token: np.array(dtype = object)
    # Output:
    #   if the token encode a keywords/the index in the file of idx, then return True
    #   otherwise, return False 
    ##
    def search_plain(self, idx, token):
        #if(len(idx) != len(token)): return False 
        return sum(
            [int(idx[i]) * int(token[i]) for i in range(len(idx))]
            ) % self.large_p == 0
    
    # Generate false positive basic tokens (unencrypted) based on hash_1 
    # Input:
    #   fp: false positive rate per hash function
    # Note: tp and fp are not overall true positive rate or false positive rate. 
    # They are aligned with the notation in the paper.
    # Output:
    #   A list of (token, bucket) pairs 
    ## 
    def gen_tokens_fp_hash_1(self, fp):
        self.set_random_seed()        
        fp_tokens_hash_1 = []
        for id in range( self.new_db_size ):
            if random.random() <= fp:
                fp_tokens_hash_1.append(
                    ( self.gen_token_basic_id_hash_1( id ), int(self.hash_1( id )) )
                )
        return fp_tokens_hash_1
    
    # Generate false positive basic tokens (unencrypted) based on hash_2
    # Input:
    #   fp: false positive rate per hash function
    # Note: tp and fp are not overall true positive rate or false positive rate. 
    # They are aligned with the notation in the paper.
    # Output:
    #   A list of (token, bucket) pairs 
    ## 
    def gen_tokens_fp_hash_2(self, fp):
        self.set_random_seed()        
        fp_tokens_hash_2 = []
        for id in range( self.new_db_size ):
            if random.random() <= fp:
                fp_tokens_hash_2.append(
                    (self.gen_token_basic_id_hash_2( id ), int( self.hash_2( id )) )
                )
        return fp_tokens_hash_2 
    
    # Generate true positive basic tokens (unencrypted)
    # Input:
    #   keyword: string 
    #   tp: true positive rate per hash function
    # Note: tp and fp are not overall true positive rate or false positive rate. 
    # They are aligned with the notation in the paper.
    # Output:
    #   A list of (token, bucket) pairs 
    ## 
    def gen_tokens_tp(self, keyword, tp):
        tp_tokens = []
        self.set_random_seed()
        for counter in range(self.countermax):
            for bucket in range(1, self.cmax + 1, 1):
                if random.random() <= tp:
                    tp_tokens.append( 
                        (self.gen_token_basic_keyword(keyword, bucket, counter), bucket)
                    )
        return tp_tokens 
    
    # Generate non-match basic tokens (unencrypted) 
    # Input:
    #   fp: false positive rate per hash function
    # Note: tp and fp are not overall true positive rate or false positive rate. 
    # They are aligned with the notation in the paper.
    # Output:
    #   A list of (token, bucket) pairs 
    ## 
    def gen_tokens_non_match(self, fp):
        nm_tokens = []
        self.set_random_seed()
        pool = [i for i in range(1, self.new_db_size + 1)]
        while len(pool) > 0:
            tmp = []
            for id in pool:
                # for hash_1 
                if random.random() <= fp:
                    nm_tokens.append(
                        ( self.gen_token_basic_padding(), id % self.cmax )
                    )
                    tmp.append( id )
            pool = tmp[:]

        return nm_tokens 


    # Generate all basic tokens (unencrypted) given a keyword 
    # Input:
    #   keyword : string 
    #   tp:  true positive rate 
    #   fp: false positive rate per hash function
    # Note: tp and fp are not overall true positive rate or false positive rate. 
    # They are aligned with the notation in the paper.
    # Output:
    #   A list of tokens 
    ## 
    def gen_tokens_plain(self, keyword, tp, fp):
        tp_tokens, fp_tokens, nm_tokens = [], [], []
        tp_tokens = self.gen_tokens_tp( keyword, tp )
        fp_tokens = self.gen_tokens_fp_hash_1( fp )
        fp_tokens +=  self.gen_tokens_fp_hash_2( fp )
        nm_tokens = self.gen_tokens_non_match( fp )

        all_tokens = tp_tokens + fp_tokens + nm_tokens
        return all_tokens 
    
    '''
    def gen_tokens_encrypted(self, keyword, tp, fp):
        tp_tokens, fp_tokens, nm_tokens = [], [], []
        tp_tokens = self.gen_tokens_tp( keyword, tp )
        fp_tokens = self.gen_tokens_fp_hash_1( fp )
        fp_tokens +=  self.gen_tokens_fp_hash_2( fp )
        nm_tokens = self.gen_tokens_non_match( fp )

        all_tokens = tp_tokens + fp_tokens + nm_tokens
        all_tokens_encrypted = []
        for tk in all_tokens:
            all_tokens_encrypted.append( 
                self.ipe.encrypt_token( tk )
             )
        return all_tokens_encrypted 
    '''
    

        

